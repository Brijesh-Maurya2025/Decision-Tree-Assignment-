{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-012\n",
        "# Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "nuHgEJK8cPUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 1:  What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "**Ans-**\n",
        "Decision Tree in Classification\n",
        "\n",
        "Definition\n",
        "\n",
        " - A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "In classification, it predicts the class label of an input by learning decision rules inferred from the data features.\n",
        "\n",
        "How It Works (Step by Step in Classification)\n",
        "\n",
        "  Root Node Creation\n",
        "\n",
        " - The algorithm starts with the entire dataset at the root.\n",
        "\n",
        " Splitting Criteria\n",
        "\n",
        " - At each node, it chooses the best feature to split on.\n",
        "\n",
        " - This is based on impurity measures like:\n",
        "\n",
        " - Gini Index (CART)\n",
        "\n",
        " - Entropy / Information Gain (ID3, C4.5)\n",
        "\n",
        " - Goal → create groups that are as pure as possible (mostly one class).\n",
        "\n",
        "Recursive Partitioning\n",
        "\n",
        " - The dataset is split recursively into child nodes.\n",
        "\n",
        " - This continues until a stopping condition is met:\n",
        "\n",
        " - Max depth reached\n",
        "\n",
        " - Minimum samples per node\n",
        "\n",
        " - Node is pure (all samples belong to one class)\n",
        "\n",
        "Leaf Nodes (Prediction)\n",
        "\n",
        " - Each leaf node corresponds to a class label.\n",
        "\n",
        " - For a new input, the model follows the rules from the root to a leaf → prediction.\n",
        "\n",
        " Example\n",
        "\n",
        "       Imagine predicting if a patient has a disease (Yes/No) using features like:\n",
        "\n",
        "       Age\n",
        "\n",
        "       Blood Pressure\n",
        "\n",
        "       Cholesterol\n",
        "\n",
        "The tree may look like:"
      ],
      "metadata": {
        "id": "0hCip0T4ccgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "          [Age > 50?]\n",
        "            /     \\\n",
        "         Yes       No\n",
        "        /            \\\n",
        " [BP > 140?]      Disease = No\n",
        "   /      \\\n",
        "Yes        No\n",
        "|          |\n",
        "Disease=Yes Disease=No\n"
      ],
      "metadata": {
        "id": "4z06VfXNi_XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n"
      ],
      "metadata": {
        "id": "pA4MSO-6cgmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Gini Impurity & Entropy in\n",
        "Decision Trees\n",
        "1. Gini Impurity\n",
        "\n",
        "  - Definition: Probability that a randomly chosen sample from the node would be misclassified if it were labeled according to the class distribution in that node.\n",
        "\n",
        "         Gini\\=1−i\\=1∑C​pi2​\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "\n",
        "- Definition: Measures the uncertainty (disorder) in a node.\n",
        "\n",
        "      “Entropy\\=−i\\=1∑C​pi​⋅log2​(pi​)”\n",
        "\n",
        "- Information Gain (IG) is used for splitting:\n",
        "\n",
        "      “IG\\=Entropy(parent)−k∑​nnk​​⋅Entropy(childk​)”\n",
        "\n",
        "\n",
        "3. Impact on Splits\n",
        "\n",
        " - At each node, the algorithm checks all possible splits across features.\n",
        "\n",
        " - It chooses the split that produces the highest reduction in impurity:\n",
        "\n",
        " - CART algorithm (Classification and Regression Trees) → uses Gini.\n",
        "\n",
        " - ID3/C4.5 algorithms → use Entropy (Information Gain).\n",
        "\n",
        " - Both measures usually lead to similar trees, but:\n",
        "\n",
        " - Gini is computationally faster (no log).\n",
        "\n",
        " - Entropy can be more sensitive when class probabilities are skewed."
      ],
      "metadata": {
        "id": "CYufA330j3lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "nm7B2Am9cm3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-  Pre-Pruning vs Post-Pruning\n",
        "\n",
        "| Aspect           | Pre-Pruning                          | Post-Pruning                              |\n",
        "| ---------------- | ------------------------------------ | ----------------------------------------- |\n",
        "| **When applied** | During tree growth                   | After full tree is built                  |\n",
        "| **Control**      | Stops growth early                   | Cuts back later                           |\n",
        "| **Risk**         | Might underfit (if pruned too early) | Safer (tree has full info before pruning) |\n",
        "| **Advantage**    | Faster, efficient                    | More accurate, interpretable              |\n",
        "\n",
        " Practical Advantage( Pre- Pruning):\n",
        "\n",
        "\n",
        "\n",
        "  - Saves training time and computational resources, especially on very large datasets.\n",
        "\n",
        "Practical Advantage(Post-Pruning):\n",
        "\n",
        "  - Produces a simpler, more interpretable tree without sacrificing much accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "NayXSoDBmra5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
      ],
      "metadata": {
        "id": "Xb0mm3ExcsK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " **Ans-**\n",
        " Information Gain in Decision Trees\n",
        "\n",
        "Definition\n",
        "\n",
        " - Information Gain (IG) measures how much uncertainty (entropy) is reduced after splitting a dataset on a feature.\n",
        "\n",
        " - In other words: it tells us how useful a feature is for classifying the data.\n",
        "\n",
        "Why It’s Important\n",
        "\n",
        " - Decision Trees work by choosing the best split at each step.\n",
        "\n",
        " - Information Gain ensures we pick the feature that:\n",
        "\n",
        " - Maximizes purity of child nodes.\n",
        "\n",
        " - Reduces uncertainty about the target variable.\n",
        "\n",
        " - Without IG (or similar measures like Gini), the tree might split on irrelevant features."
      ],
      "metadata": {
        "id": "6z-pGpGgn2uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Dataset Info:**\n",
        "    \n",
        "    ● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "    provided CSV).\n",
        "    ● Boston Housing Dataset for regression tasks\n",
        "    (sklearn.datasets.load_boston() or provided CSV)."
      ],
      "metadata": {
        "id": "yNrzpuu6c0VC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-   \n",
        "**Real-World Applications of Decision Trees**\n",
        "\n",
        "1. Common Applications\n",
        "\n",
        "Classification\n",
        "\n",
        " - Healthcare → Predict whether a patient has a disease (Yes/No).\n",
        "\n",
        " - Finance → Fraud detection in credit card transactions.\n",
        "\n",
        " - Customer Analytics → Predict whether a customer will churn or not.\n",
        "\n",
        " - Education → Classify students as \"pass/fail\" based on grades.\n",
        "\n",
        "Dataset Example: Iris (Classification)\n",
        "\n",
        " - Features: Petal length, petal width, sepal length, sepal width.\n",
        "\n",
        " - Task: Classify flowers into Setosa, Versicolor, Virginica.\n",
        "\n",
        " - A Decision Tree learns rules like:\n",
        "\n",
        "arduino\n",
        "\n",
        "       if petal_length < 2.5 → Setosa  \n",
        "       else if petal_width < 1.75 → Versicolor  \n",
        "       else → Virginica\n",
        "\n",
        "Regression\n",
        "\n",
        " - Real Estate → Predict housing prices.\n",
        "\n",
        " - Economics → Predict GDP growth or demand forecasting.\n",
        "\n",
        " - Healthcare → Predict length of hospital stay.\n",
        "\n",
        " - Agriculture → Estimate crop yield.\n",
        "\n",
        "Dataset Example: Boston Housing (Regression)\n",
        "\n",
        " - Features: Rooms per house, crime    rate, proximity to jobs, etc.\n",
        "\n",
        " - Task: Predict median house value.\n",
        "\n",
        " - A Decision Tree regressor splits data into regions and assigns the average house price of each region as prediction.\n",
        "\n",
        "2. Advantages of Decision Trees\n",
        "\n",
        " - Interpretability – Easy to visualize (flowchart style) and explain to non-technical people.\n",
        "\n",
        " - Handles Mixed Data – Works with both categorical & numerical features.\n",
        "\n",
        " - No Feature Scaling Needed – Unlike SVM or Logistic Regression.\n",
        "\n",
        " - Captures Nonlinear Relationships – Flexible decision boundaries.\n",
        "\n",
        " - Versatile – Can be used for both classification (Iris) and regression (Boston Housing).\n",
        "\n",
        "3. Limitations of Decision Trees\n",
        "\n",
        " - Overfitting – Trees can grow too deep and memorize training data (needs pruning or ensembles).\n",
        "\n",
        " - Instability – Small changes in data can lead to very different trees.\n",
        "\n",
        " - Bias Toward Features with Many Categories – Categorical variables with many levels can dominate.\n",
        "\n",
        " - Lower Predictive Accuracy – A single tree is weaker compared to ensemble methods like Random Forests or Gradient Boosted Trees."
      ],
      "metadata": {
        "id": "4LxgmJqPqcuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:   Write a Python program to:\n",
        "\n",
        "    ● Load the Iris Dataset\n",
        "    ● Train a Decision Tree Classifier using the Gini criterion\n",
        "    ● Print the model’s accuracy and feature importances\n",
        "    (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "DZKTgudec368"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans-\n",
        "# Decision Tree Classifier on Iris Dataset\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 6. Get feature importances\n",
        "feature_importances = pd.Series(clf.feature_importances_, index=feature_names)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Classifier (Gini Criterion)\")\n",
        "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tGTqDkHvExd",
        "outputId": "4f5fba2a-4661-4a9e-acae-1395f590f955"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier (Gini Criterion)\n",
            "Accuracy on test set: 93.33%\n",
            "\n",
            "Feature Importances:\n",
            "petal length (cm)    0.558568\n",
            "petal width (cm)     0.406015\n",
            "sepal width (cm)     0.029167\n",
            "sepal length (cm)    0.006250\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:  Write a Python program to:**\n",
        "\n",
        "    ● Load the Iris Dataset\n",
        "    ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "    a fully-grown tree.\n",
        "    (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "FCtOaOp3dXwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans-\n",
        "# Compare pruned vs fully-grown Decision Tree on Iris Dataset\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train a pruned Decision Tree (max_depth = 3)\n",
        "clf_pruned = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# 4. Train a fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 5. Print results\n",
        "print(\"Decision Tree Classifier Comparison on Iris Dataset\")\n",
        "print(\"Pruned Tree (max_depth=3) Accuracy: {:.2f}%\".format(accuracy_pruned * 100))\n",
        "print(\"Fully-grown Tree Accuracy: {:.2f}%\".format(accuracy_full * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzzcUDfSvg2k",
        "outputId": "0719652d-ebfc-4f90-e274-9cc294da3e84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Comparison on Iris Dataset\n",
            "Pruned Tree (max_depth=3) Accuracy: 96.67%\n",
            "Fully-grown Tree Accuracy: 93.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        "    ● Load the California Housing dataset from sklearn\n",
        "    ● Train a Decision Tree Regressor\n",
        "    ● Print the Mean Squared Error (MSE) and feature importances\n",
        "    (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "xmJWABH8djHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans-\n",
        "# Decision Tree Regressor on California Housing Dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# 6. Get feature importances\n",
        "feature_importances = pd.Series(regressor.feature_importances_, index=feature_names)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Regressor on California Housing Dataset\")\n",
        "print(\"Mean Squared Error (MSE): {:.4f}\".format(mse))\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgNlvbkgvtEJ",
        "outputId": "0dffb858-5a98-4d0d-ed32-6a1cac7fdbcf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor on California Housing Dataset\n",
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "MedInc        0.528509\n",
            "AveOccup      0.130838\n",
            "Latitude      0.093717\n",
            "Longitude     0.082902\n",
            "AveRooms      0.052975\n",
            "HouseAge      0.051884\n",
            "Population    0.030516\n",
            "AveBedrms     0.028660\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "\n",
        "    ● Load the Iris Dataset\n",
        "    ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "    GridSearchCV\n",
        "    ● Print the best parameters and the resulting model accuracy\n",
        "    (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "fcSJg_E9dsmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans-\n",
        "# Hyperparameter tuning with GridSearchCV on Iris Dataset\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 4. Create GridSearchCV with Decision Tree Classifier\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5, scoring='accuracy'\n",
        ")\n",
        "\n",
        "# 5. Fit the grid search to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# 7. Evaluate best model on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Classifier with GridSearchCV (Iris Dataset)\")\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFgxiEtav-g8",
        "outputId": "c89cb6af-ede6-4a58-a5fd-1645374d3d86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier with GridSearchCV (Iris Dataset)\n",
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy on test set: 93.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.**\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "    \n",
        "    ● Handle the missing values\n",
        "    ● Encode the categorical features\n",
        "    ● Train a Decision Tree model\n",
        "    ● Tune its hyperparameters\n",
        "    ● Ev aluate its performance\n",
        "    And describe what business value this model could provide in the real-world\n",
        "    setting."
      ],
      "metadata": {
        "id": "XlBG1OSUd04s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans**-  Step-by-Step Process\n",
        "\n",
        "1. Handle the Missing Values\n",
        "\n",
        "    Explore Missingness\n",
        "\n",
        "    - Check % of missing values per column.\n",
        "\n",
        "   -  Identify whether missingness is MCAR (Missing Completely at Random), MAR (Missing at Random), or MNAR (Missing Not at Random).\n",
        "\n",
        " Strategies:\n",
        "\n",
        "  - Numerical features → Use mean/median imputation (median is robust to outliers).\n",
        "\n",
        "   -  Categorical features → Use mode imputation or add a new category like \"Unknown\".\n",
        "\n",
        "   - Advanced methods → Use KNN imputation or multivariate imputation (MICE) if dataset is large.\n",
        "\n",
        "2. Encode the Categorical Features\n",
        "\n",
        "   -  Ordinal categorical (e.g., disease stage: low, medium, high) → Use Ordinal Encoding.\n",
        "\n",
        "  -  Nominal categorical (e.g., gender, blood group) → Use One-Hot Encoding.\n",
        "\n",
        "   -  If dataset has high-cardinality features (like ZIP codes, hospital IDs):\n",
        "\n",
        "  - Use Target Encoding or Frequency Encoding.\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "\n",
        "  - Data Split → Train/Test (e.g., 80/20) or use Stratified K-Fold Cross Validation (because class imbalance is common in healthcare).\n",
        "\n",
        "  - Scaling is not required for Decision Trees.\n",
        "\n",
        "  - Train initial Decision Tree using scikit-learn:"
      ],
      "metadata": {
        "id": "469OqKn6e_bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "GMLBU7DOgrsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tune Hyperparameters\n",
        "\n",
        " - Use GridSearchCV or RandomizedSearchCV to optimize:\n",
        "\n",
        " - max_depth: to control tree growth.\n",
        "\n",
        " - min_samples_split, min_samples_leaf: to avoid overfitting.\n",
        "\n",
        " - criterion: \"gini\" vs \"entropy\".\n",
        "\n",
        " - max_features: number of features to consider for best split.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "7e5AhCUGg6h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                    param_grid, cv=5, scoring='roc_auc')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n"
      ],
      "metadata": {
        "id": "E-or9F4ZhF1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate Model Performance\n",
        "\n",
        " - Since healthcare is high-risk (false negatives can be critical):\n",
        "\n",
        " - Use metrics beyond accuracy:\n",
        "\n",
        " - Confusion Matrix\n",
        "\n",
        " - Precision, Recall, F1-score\n",
        "\n",
        " - ROC-AUC Score\n",
        "\n",
        " - PR Curve (important when classes are imbalanced).\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "jS0j09M5hLsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))\n"
      ],
      "metadata": {
        "id": "8aZAOZZ0hXoU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}